# Grafana Q Devç”¨æˆ·æŒ‡æ ‡æ•°æ®é›†æˆæ–¹æ¡ˆ

## ğŸ“Š æ•°æ®æºåˆ†æ

### Grafanaä»ªè¡¨æ¿
- **URL**: http://<EC2-PUBLIC-IP>:3000/d/qdev_user_metrics/q-dev-user-metrics-dashboard
- **æ•°æ®æº**: DevLake MySQLæ•°æ®åº“
- **ä¸»è¦æ•°æ®è¡¨**:
  - `_tool_q_dev_user_metrics` - ç”¨æˆ·èšåˆæŒ‡æ ‡
  - `_tool_q_dev_user_data` - ç”¨æˆ·æ—¥å¸¸æ•°æ®

### æ ¸å¿ƒæŒ‡æ ‡å­—æ®µ
```sql
-- ç”¨æˆ·èšåˆæŒ‡æ ‡è¡¨ (_tool_q_dev_user_metrics)
- user_id, display_name                    -- ç”¨æˆ·æ ‡è¯†
- first_date, last_date, total_days        -- æ—¶é—´èŒƒå›´
- total_inline_suggestions_count           -- æ€»å»ºè®®æ•°
- total_inline_acceptance_count            -- æ€»æ¥å—æ•°
- acceptance_rate                          -- æ¥å—ç‡
- total_inline_ai_code_lines              -- AIç”Ÿæˆä»£ç è¡Œæ•°
- total_code_review_findings_count         -- ä»£ç å®¡æŸ¥å‘ç°æ•°
- avg_* å­—æ®µ                              -- å„ç§å¹³å‡å€¼

-- ç”¨æˆ·æ—¥å¸¸æ•°æ®è¡¨ (_tool_q_dev_user_data)
- user_id, display_name, date             -- ç”¨æˆ·å’Œæ—¥æœŸ
- inline_suggestions_count                -- æ—¥å»ºè®®æ•°
- inline_acceptance_count                 -- æ—¥æ¥å—æ•°
- chat_messages_sent                      -- èŠå¤©æ¶ˆæ¯æ•°
- code_fix_generation_event_count         -- ä»£ç ä¿®å¤ç”Ÿæˆäº‹ä»¶
- test_generation_event_count             -- æµ‹è¯•ç”Ÿæˆäº‹ä»¶
```

## ğŸ”„ æ•°æ®é›†æˆæ–¹æ¡ˆ

### æ–¹æ¡ˆ1: ç›´æ¥æ•°æ®åº“è®¿é—® (æœ€ç®€å•)

#### ç‰¹ç‚¹
- âœ… å®ç°ç®€å•ï¼Œå»¶è¿Ÿæœ€ä½
- âœ… æ•°æ®ä¸€è‡´æ€§æœ€å¥½
- âŒ éœ€è¦æ•°æ®åº“è®¿é—®æƒé™
- âŒ ä¸DevLakeç³»ç»Ÿè€¦åˆ

#### å®ç°æ–¹å¼
```python
import mysql.connector
import pandas as pd

# æ•°æ®åº“è¿æ¥
config = {
    'host': '<EC2-PUBLIC-IP>',
    'port': 3306,
    'user': 'merico',
    'password': 'merico',
    'database': 'lake'
}

def get_user_metrics(start_date, end_date):
    conn = mysql.connector.connect(**config)
    
    # èšåˆæŒ‡æ ‡æŸ¥è¯¢
    query = """
    SELECT 
        user_id,
        display_name,
        total_inline_suggestions_count,
        total_inline_acceptance_count,
        acceptance_rate,
        total_inline_ai_code_lines,
        avg_inline_suggestions_count,
        first_date,
        last_date
    FROM _tool_q_dev_user_metrics
    WHERE connection_id = 1
    """
    
    df = pd.read_sql(query, conn)
    conn.close()
    return df

# ä½¿ç”¨ç¤ºä¾‹
metrics_data = get_user_metrics('2025-09-15', '2025-09-17')
```

#### é€‚ç”¨åœºæ™¯
- å†…éƒ¨æŠ¥è¡¨ç³»ç»Ÿ
- å®æ—¶æ•°æ®éœ€æ±‚
- æŠ€æœ¯å›¢é˜Ÿæœ‰æ•°æ®åº“è®¿é—®èƒ½åŠ›

---

### æ–¹æ¡ˆ2: DevLake APIæ¥å£ (æ¨è)

#### ç‰¹ç‚¹
- âœ… æ ‡å‡†åŒ–APIæ¥å£
- âœ… æƒé™æ§åˆ¶å®Œå–„
- âœ… æ•°æ®æ ¼å¼ç»Ÿä¸€
- âŒ éœ€è¦APIå¼€å‘

#### å®ç°æ–¹å¼
```python
import requests
import json

class DevLakeAPI:
    def __init__(self, base_url="http://<EC2-PUBLIC-IP>:8080"):
        self.base_url = base_url
    
    def get_q_dev_metrics(self, connection_id=1):
        """è·å–Q Devç”¨æˆ·æŒ‡æ ‡"""
        url = f"{self.base_url}/plugins/q_dev/connections/{connection_id}/metrics"
        response = requests.get(url)
        return response.json()
    
    def get_user_data(self, connection_id=1, start_date=None, end_date=None):
        """è·å–ç”¨æˆ·æ—¥å¸¸æ•°æ®"""
        url = f"{self.base_url}/plugins/q_dev/connections/{connection_id}/user-data"
        params = {}
        if start_date:
            params['start_date'] = start_date
        if end_date:
            params['end_date'] = end_date
            
        response = requests.get(url, params=params)
        return response.json()

# ä½¿ç”¨ç¤ºä¾‹
api = DevLakeAPI()
metrics = api.get_q_dev_metrics()
daily_data = api.get_user_data(start_date='2025-09-15', end_date='2025-09-17')
```

#### é€‚ç”¨åœºæ™¯
- ç¬¬ä¸‰æ–¹ç³»ç»Ÿé›†æˆ
- éœ€è¦æƒé™æ§åˆ¶
- æ ‡å‡†åŒ–æ•°æ®æ¥å£éœ€æ±‚

---

### æ–¹æ¡ˆ3: Grafana APIå¯¼å‡º (ä¸­ç­‰å¤æ‚åº¦)

#### ç‰¹ç‚¹
- âœ… åˆ©ç”¨ç°æœ‰Grafanaé…ç½®
- âœ… æ”¯æŒå¤šç§æ•°æ®æ ¼å¼
- âŒ éœ€è¦Grafanaè®¤è¯
- âŒ æ•°æ®æ ¼å¼å¯èƒ½éœ€è¦è½¬æ¢

#### å®ç°æ–¹å¼
```python
import requests
import base64

class GrafanaExporter:
    def __init__(self, grafana_url="http://<EC2-PUBLIC-IP>:3000"):
        self.grafana_url = grafana_url
        self.session = requests.Session()
        
    def login(self, username="admin", password="admin"):
        """ç™»å½•Grafana"""
        login_data = {
            "user": username,
            "password": password
        }
        response = self.session.post(f"{self.grafana_url}/login", data=login_data)
        return response.status_code == 200
    
    def export_dashboard_data(self, dashboard_uid="qdev_user_metrics", 
                            from_time="now-2d", to_time="now"):
        """å¯¼å‡ºä»ªè¡¨æ¿æ•°æ®"""
        # è·å–ä»ªè¡¨æ¿é…ç½®
        dashboard_url = f"{self.grafana_url}/api/dashboards/uid/{dashboard_uid}"
        dashboard = self.session.get(dashboard_url).json()
        
        # å¯¼å‡ºé¢æ¿æ•°æ®
        panels_data = []
        for panel in dashboard['dashboard']['panels']:
            if 'targets' in panel:
                for target in panel['targets']:
                    # æ‰§è¡ŒæŸ¥è¯¢
                    query_url = f"{self.grafana_url}/api/ds/query"
                    query_data = {
                        "queries": [target],
                        "from": from_time,
                        "to": to_time
                    }
                    result = self.session.post(query_url, json=query_data)
                    panels_data.append({
                        'panel_title': panel.get('title', ''),
                        'data': result.json()
                    })
        
        return panels_data

# ä½¿ç”¨ç¤ºä¾‹
exporter = GrafanaExporter()
exporter.login()
data = exporter.export_dashboard_data()
```

#### é€‚ç”¨åœºæ™¯
- éœ€è¦ä¿æŒä¸Grafanaä¸€è‡´çš„æ•°æ®è§†å›¾
- å¤ç”¨ç°æœ‰ä»ªè¡¨æ¿é…ç½®
- å®šæœŸæ•°æ®å¯¼å‡ºéœ€æ±‚

---

### æ–¹æ¡ˆ4: æ•°æ®æ–‡ä»¶å¯¼å‡º (æ‰¹å¤„ç†)

#### ç‰¹ç‚¹
- âœ… é€‚åˆå¤§æ‰¹é‡æ•°æ®å¤„ç†
- âœ… æ”¯æŒå¤šç§æ–‡ä»¶æ ¼å¼
- âœ… å¯ä»¥ç¦»çº¿å¤„ç†
- âŒ æ•°æ®ä¸æ˜¯å®æ—¶çš„
- âŒ éœ€è¦å®šæœŸåŒæ­¥

#### å®ç°æ–¹å¼
```python
import pandas as pd
import json
from datetime import datetime, timedelta

class DataExporter:
    def __init__(self, db_config):
        self.db_config = db_config
    
    def export_to_csv(self, output_path="qdev_metrics.csv"):
        """å¯¼å‡ºä¸ºCSVæ–‡ä»¶"""
        conn = mysql.connector.connect(**self.db_config)
        
        query = """
        SELECT 
            m.user_id,
            m.display_name,
            m.total_inline_suggestions_count,
            m.total_inline_acceptance_count,
            m.acceptance_rate,
            m.total_inline_ai_code_lines,
            m.first_date,
            m.last_date,
            d.date,
            d.inline_suggestions_count as daily_suggestions,
            d.inline_acceptance_count as daily_acceptance,
            d.chat_messages_sent
        FROM _tool_q_dev_user_metrics m
        LEFT JOIN _tool_q_dev_user_data d ON m.user_id = d.user_id
        WHERE m.connection_id = 1
        ORDER BY m.user_id, d.date
        """
        
        df = pd.read_sql(query, conn)
        df.to_csv(output_path, index=False)
        conn.close()
        return output_path
    
    def export_to_json(self, output_path="qdev_metrics.json"):
        """å¯¼å‡ºä¸ºJSONæ–‡ä»¶"""
        conn = mysql.connector.connect(**self.db_config)
        
        # ç”¨æˆ·èšåˆæ•°æ®
        users_query = "SELECT * FROM _tool_q_dev_user_metrics WHERE connection_id = 1"
        users_df = pd.read_sql(users_query, conn)
        
        # æ—¥å¸¸æ•°æ®
        daily_query = "SELECT * FROM _tool_q_dev_user_data WHERE connection_id = 1"
        daily_df = pd.read_sql(daily_query, conn)
        
        # ç»„åˆæ•°æ®
        export_data = {
            "export_time": datetime.now().isoformat(),
            "user_metrics": users_df.to_dict('records'),
            "daily_data": daily_df.to_dict('records')
        }
        
        with open(output_path, 'w') as f:
            json.dump(export_data, f, indent=2, default=str)
        
        conn.close()
        return output_path

# å®šæ—¶ä»»åŠ¡ç¤ºä¾‹ (ä½¿ç”¨cronæˆ–è°ƒåº¦å™¨)
def daily_export_job():
    exporter = DataExporter(db_config)
    csv_file = exporter.export_to_csv(f"qdev_metrics_{datetime.now().strftime('%Y%m%d')}.csv")
    json_file = exporter.export_to_json(f"qdev_metrics_{datetime.now().strftime('%Y%m%d')}.json")
    
    # ä¸Šä¼ åˆ°S3æˆ–å…¶ä»–å­˜å‚¨
    # upload_to_s3(csv_file, json_file)
```

#### é€‚ç”¨åœºæ™¯
- æ•°æ®ä»“åº“ETLæµç¨‹
- ç¦»çº¿åˆ†æéœ€æ±‚
- å¤§æ‰¹é‡æ•°æ®å¤„ç†

---

### æ–¹æ¡ˆ5: å®æ—¶æ•°æ®æµ (æœ€å¤æ‚)

#### ç‰¹ç‚¹
- âœ… çœŸæ­£çš„å®æ—¶æ•°æ®
- âœ… æ”¯æŒæµå¼å¤„ç†
- âœ… å¯æ‰©å±•æ€§å¥½
- âŒ å®ç°å¤æ‚åº¦é«˜
- âŒ éœ€è¦é¢å¤–åŸºç¡€è®¾æ–½

#### å®ç°æ–¹å¼
```python
# ä½¿ç”¨Kafka + MySQL Binlog
from kafka import KafkaProducer, KafkaConsumer
import json
from pymysqlreplication import BinLogStreamReader
from pymysqlreplication.row_event import WriteRowsEvent, UpdateRowsEvent

class MySQLStreamer:
    def __init__(self, kafka_bootstrap_servers=['localhost:9092']):
        self.producer = KafkaProducer(
            bootstrap_servers=kafka_bootstrap_servers,
            value_serializer=lambda v: json.dumps(v, default=str).encode('utf-8')
        )
    
    def stream_mysql_changes(self):
        """ç›‘å¬MySQLå˜æ›´å¹¶å‘é€åˆ°Kafka"""
        stream = BinLogStreamReader(
            connection_settings={
                'host': '<EC2-PUBLIC-IP>',
                'port': 3306,
                'user': 'merico',
                'passwd': 'merico'
            },
            server_id=100,
            only_events=[WriteRowsEvent, UpdateRowsEvent],
            only_tables=['_tool_q_dev_user_metrics', '_tool_q_dev_user_data']
        )
        
        for binlogevent in stream:
            for row in binlogevent.rows:
                event_data = {
                    'table': binlogevent.table,
                    'event_type': binlogevent.__class__.__name__,
                    'data': row['values'] if hasattr(row, 'values') else row['after_values'],
                    'timestamp': binlogevent.timestamp
                }
                
                # å‘é€åˆ°Kafka
                self.producer.send('qdev_metrics_stream', event_data)

class StreamConsumer:
    def __init__(self, kafka_bootstrap_servers=['localhost:9092']):
        self.consumer = KafkaConsumer(
            'qdev_metrics_stream',
            bootstrap_servers=kafka_bootstrap_servers,
            value_deserializer=lambda m: json.loads(m.decode('utf-8'))
        )
    
    def process_stream(self, callback_func):
        """å¤„ç†æ•°æ®æµ"""
        for message in self.consumer:
            data = message.value
            callback_func(data)

# ä½¿ç”¨ç¤ºä¾‹
def handle_metrics_update(data):
    """å¤„ç†æŒ‡æ ‡æ›´æ–°"""
    if data['table'] == '_tool_q_dev_user_metrics':
        # æ›´æ–°ç¬¬ä¸‰æ–¹ç³»ç»Ÿ
        update_external_system(data['data'])

streamer = MySQLStreamer()
consumer = StreamConsumer()

# å¯åŠ¨æµå¤„ç†
import threading
threading.Thread(target=streamer.stream_mysql_changes).start()
consumer.process_stream(handle_metrics_update)
```

#### é€‚ç”¨åœºæ™¯
- å®æ—¶ç›‘æ§ç³»ç»Ÿ
- é«˜é¢‘æ•°æ®æ›´æ–°éœ€æ±‚
- å¤§è§„æ¨¡åˆ†å¸ƒå¼ç³»ç»Ÿ

---

## ğŸ“‹ æ–¹æ¡ˆå¯¹æ¯”æ€»ç»“

| æ–¹æ¡ˆ | å¤æ‚åº¦ | å®æ—¶æ€§ | æ•°æ®ä¸€è‡´æ€§ | ç»´æŠ¤æˆæœ¬ | é€‚ç”¨åœºæ™¯ |
|------|--------|--------|------------|----------|----------|
| ç›´æ¥æ•°æ®åº“è®¿é—® | â­ | â­â­â­â­â­ | â­â­â­â­â­ | â­â­ | å†…éƒ¨ç³»ç»Ÿï¼Œå®æ—¶éœ€æ±‚ |
| DevLake API | â­â­ | â­â­â­â­ | â­â­â­â­ | â­â­â­ | æ ‡å‡†é›†æˆï¼Œæ¨èæ–¹æ¡ˆ |
| Grafana API | â­â­â­ | â­â­â­ | â­â­â­ | â­â­â­ | å¤ç”¨ç°æœ‰é…ç½® |
| æ–‡ä»¶å¯¼å‡º | â­â­ | â­ | â­â­â­â­ | â­â­ | æ‰¹å¤„ç†ï¼Œç¦»çº¿åˆ†æ |
| å®æ—¶æ•°æ®æµ | â­â­â­â­â­ | â­â­â­â­â­ | â­â­â­â­ | â­â­â­â­â­ | å¤§è§„æ¨¡å®æ—¶ç³»ç»Ÿ |

## ğŸ¯ æ¨èå®æ–½è·¯å¾„

### é˜¶æ®µ1: å¿«é€Ÿå¯åŠ¨ (1-2å‘¨)
é€‰æ‹©**æ–¹æ¡ˆ2: DevLake APIæ¥å£**
- å¼€å‘æ ‡å‡†REST API
- å®ç°åŸºç¡€æ•°æ®æŸ¥è¯¢åŠŸèƒ½
- æä¾›JSONæ ¼å¼æ•°æ®è¾“å‡º

### é˜¶æ®µ2: åŠŸèƒ½å¢å¼º (2-4å‘¨)
ç»“åˆ**æ–¹æ¡ˆ4: æ•°æ®æ–‡ä»¶å¯¼å‡º**
- æ·»åŠ å®šæ—¶æ‰¹é‡å¯¼å‡ºåŠŸèƒ½
- æ”¯æŒå¤šç§æ•°æ®æ ¼å¼ (CSV, JSON, Excel)
- å®ç°æ•°æ®å†å²å½’æ¡£

### é˜¶æ®µ3: é«˜çº§ç‰¹æ€§ (1-2æœˆ)
æ ¹æ®éœ€æ±‚é€‰æ‹©**æ–¹æ¡ˆ3æˆ–æ–¹æ¡ˆ5**
- å¦‚éœ€å¤ç”¨Grafanaé…ç½®ï¼Œé€‰æ‹©æ–¹æ¡ˆ3
- å¦‚éœ€çœŸæ­£å®æ—¶æ•°æ®ï¼Œé€‰æ‹©æ–¹æ¡ˆ5

## ğŸ”§ æŠ€æœ¯å®ç°å»ºè®®

### APIè®¾è®¡è§„èŒƒ
```
GET /api/v1/qdev/metrics/users              # è·å–ç”¨æˆ·åˆ—è¡¨
GET /api/v1/qdev/metrics/users/{user_id}    # è·å–ç‰¹å®šç”¨æˆ·æŒ‡æ ‡
GET /api/v1/qdev/metrics/daily              # è·å–æ—¥å¸¸æ•°æ®
GET /api/v1/qdev/metrics/export             # å¯¼å‡ºæ•°æ®
```

### æ•°æ®æ ¼å¼æ ‡å‡†
```json
{
  "user_id": "<USER-ID-1>",
  "display_name": "User Name",
  "metrics": {
    "total_suggestions": 6,
    "total_acceptance": 3,
    "acceptance_rate": 0.5,
    "ai_code_lines": 3,
    "active_days": 1
  },
  "daily_data": [
    {
      "date": "2025-09-15",
      "suggestions": 6,
      "acceptance": 3,
      "chat_messages": 0
    }
  ]
}
```

è¿™å¥—æ–¹æ¡ˆå¯ä»¥æ ¹æ®å…·ä½“éœ€æ±‚å’ŒæŠ€æœ¯èƒ½åŠ›é€‰æ‹©åˆé€‚çš„å®æ–½è·¯å¾„ï¼Œä»ç®€å•åˆ°å¤æ‚é€æ­¥æ¼”è¿›ã€‚
